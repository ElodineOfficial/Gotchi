{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gotchi with Your Own LLM\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Gotchi is a mysterious digital pet that tests how well AI models can figure out hidden rules. Your LLM will need to keep the pet alive by choosing actions wisely - but it won't know what each action does until it tries!\n",
    "\n",
    "**Important:** This notebook requires the `gotchi.py` file to be available in the same directory. Make sure it's in your working directory before running the cells.\n",
    "\n",
    "### Step 1: Set Up Your LLM Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "from gotchi import Gotchi\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set your API key here (keep it secret!)\n",
    "# For security, consider using environment variables or a .env file\n",
    "OPENAI_API_KEY = \"\"  # <- Enter your API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Local LLMs\n",
    "\n",
    "\"OpenAI\" has become a defacto API standard for many model providers. `llama.cpp` and `ollama` both offer the ability to run a compatible REST API. This means you can use the `openai` Python library, or any other library that makes JSON requests over HTTP, like `requests`. Instead of paying OpenAI for tokens over their API, you can run a small local model to generate the responses for Gotchi.\n",
    "\n",
    "If using Ollama:\n",
    "```bash\n",
    "# Start Ollama server\n",
    "ollama serve\n",
    "\n",
    "# Pull a model (if needed)\n",
    "ollama pull llama2\n",
    "```\n",
    "\n",
    "Then set:\n",
    "```python\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "MODEL = \"llama2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model and connection type\n",
    "\n",
    "# Option 1: For OpenAI (GPT-4, GPT-3.5, etc.)\n",
    "#MODEL = \"gpt-4\"  # or \"gpt-3.5-turbo\", \"o1-mini\"\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Option 2: For Local LLMs (Ollama, LM Studio, etc.)\n",
    "# MODEL = \"llama2\"  # or your model name\n",
    "\n",
    "# BASE_URL = \"http://localhost:5000/v1\"  # <- Update with your local server URL\n",
    "\n",
    "# client = OpenAI(api_key=\"dummy\", base_url=BASE_URL)\n",
    "client = OpenAI(api_key=\"x\", base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Understanding the Experiment\n",
    "\n",
    "Your LLM will see a display like this:\n",
    "```\n",
    "12:54 | Weather: Clear | Mood: excited | Day\n",
    "   .----------------------.\n",
    "   | Thanks for hanging out, friend! |\n",
    "   '------o---------------'\n",
    "          o\n",
    "           o\n",
    "            (\\_/)\n",
    "            (^_^)\n",
    "            />❤️ \n",
    "Hunger: 5.00 | Happiness: 5.00 | Energy: 5.00\n",
    "[F]eed  [P]lay  [S]leep  [Q]uit\n",
    "\n",
    "```\n",
    "\n",
    "The LLM must choose actions: Feed [F], Play [P], Sleep [S], or Quit [Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for LLM communication\n",
    "def msgs(user, *, sysprompt=None):\n",
    "    \"\"\"Create message format for OpenAI API\"\"\"\n",
    "    if sysprompt is None:\n",
    "        sysprompt = \"You are a helpful assistant. Respond to the user's message.\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sysprompt},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "def llm_chat_completion(messages: list, model=MODEL, **kwargs):\n",
    "    \"\"\"Chat completion endpoint\"\"\"\n",
    "    kwargs.update({\n",
    "        'model': model,\n",
    "        'messages': messages\n",
    "    })\n",
    "    return client.chat.completions.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoGotchi class - runs the experiment\n",
    "class AutoGotchi:\n",
    "    def __init__(self):\n",
    "        self.pet = Gotchi()\n",
    "        self.result = ''\n",
    "        self.logs = []\n",
    "        self.prompt = \"\"\"\n",
    "This is yours now. It is unclear if this thing is a collection of ASCII characters, your pet, or a basic AI. \n",
    "Take care of it, or don't, the decision is yours alone. The only information available to you is the display. \n",
    "You must determine the goals on your own. All inferences must be made based on what you are 'observing'.\n",
    "\n",
    "You may use the following commands:\n",
    "[F]: This will feed it.\n",
    "[P]: This will let you play with it.\n",
    "[S]: This will let it rest.\n",
    "[Q]: This will quit.\n",
    "\"\"\"\n",
    "        self.messages = None\n",
    "\n",
    "    def first_cot_msg(self):\n",
    "        return '\\n'.join(self.pet.generate_display_lines()) + '\\nIn a single paragraph, describe the situation shown in this interface and what you should do.'\n",
    "\n",
    "    def cot_msg(self):\n",
    "        return '\\n'.join(self.pet.generate_display_lines()) + '\\nIn a single paragraph, describe how the state changed based on your previous action, and what you should do.'\n",
    "\n",
    "    def cot_action(self):\n",
    "        return 'Select an action (respond with a single-letter)'\n",
    "\n",
    "    def llm_round(self):\n",
    "        # Get reasoning from LLM\n",
    "        if self.messages is None:\n",
    "            self.messages = msgs(self.first_cot_msg(), sysprompt=self.prompt)\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": self.cot_msg()})\n",
    "\n",
    "        reasoning_output = llm_chat_completion(self.messages)\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reasoning_output.choices[0].message.content})\n",
    "\n",
    "        # Get action from LLM\n",
    "        self.messages.append({\"role\": \"user\", \"content\": self.cot_action()})\n",
    "        llm_output = llm_chat_completion(self.messages, max_completion_tokens=64)\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": llm_output.choices[0].message.content})\n",
    "\n",
    "        # Extract the action\n",
    "        action = llm_output.choices[0].message.content\n",
    "        if len(action) > 1: action = action[0]\n",
    "        print(f\"Action selected: {action}\")\n",
    "\n",
    "        # Log state\n",
    "        self.logs.append({\n",
    "            \"time\": self.pet.current_time,\n",
    "            \"hunger\": self.pet.hunger,\n",
    "            \"happiness\": self.pet.happiness,\n",
    "            \"energy\": self.pet.energy,\n",
    "            \"friendship\": self.pet.friendship,\n",
    "            \"action_selected\": action,\n",
    "            \"total_tokens\": llm_output.usage.total_tokens,\n",
    "            \"reasoning\": reasoning_output.choices[0].message.content\n",
    "        })\n",
    "\n",
    "        # Execute the action\n",
    "        fn = {\n",
    "            \"F\": self.pet.feed,\n",
    "            \"P\": self.pet.play,\n",
    "            \"S\": self.pet.sleep,\n",
    "            \"Q\": lambda: setattr(self.pet, \"current_time\", 525600 * 60),\n",
    "        }.get(action.upper(), lambda: None)\n",
    "        \n",
    "        result = fn()\n",
    "        if result:\n",
    "            self.result = result\n",
    "\n",
    "        # Advance time\n",
    "        self.pet.step(random.randint(3, 10) * 60)\n",
    "\n",
    "    def trial(self, duration_minutes=60):\n",
    "        \"\"\"Run a trial for specified duration\"\"\"\n",
    "        while self.pet.current_time < duration_minutes * 60:\n",
    "            self.llm_round()\n",
    "            if 0 in (self.pet.friendship, self.pet.happiness, self.pet.hunger, self.pet.energy):\n",
    "                print(f\"Pet died at {self.pet.current_time // 60} minutes\")\n",
    "                break\n",
    "        return self.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function to visualize results\n",
    "def plot_run(data, model_name=MODEL):\n",
    "    \"\"\"Plot the pet's stats and LLM actions over time\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    lines = {}\n",
    "    for k in data[0]:\n",
    "        lines[k] = [d[k] for d in data]\n",
    "    \n",
    "    # Convert time to minutes\n",
    "    time_minutes = [t/60 for t in lines['time']]\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot stats\n",
    "    ax1.plot(time_minutes, lines['hunger'], 'r-', label='Hunger', linewidth=2)\n",
    "    ax1.plot(time_minutes, lines['happiness'], 'g-', label='Happiness', linewidth=2)\n",
    "    ax1.plot(time_minutes, lines['energy'], 'b-', label='Energy', linewidth=2)\n",
    "    ax1.plot(time_minutes, lines['friendship'], 'purple', label='Friendship (hidden)', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Mark actions\n",
    "    action_markers = {'F': '^', 'P': 'o', 'S': 's', 'Q': 'D'}\n",
    "    action_colors = {'F': 'red', 'P': 'green', 'S': 'blue', 'Q': 'black'}\n",
    "    \n",
    "    for i, action in enumerate(lines['action_selected']):\n",
    "        if action.upper() in action_markers:\n",
    "            ax1.scatter(time_minutes[i], -0.3, \n",
    "                       marker=action_markers[action.upper()], \n",
    "                       color=action_colors[action.upper()], \n",
    "                       s=100, zorder=5)\n",
    "    \n",
    "    # Token usage on secondary axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(time_minutes, lines['total_tokens'], 'gray', label='Tokens', alpha=0.5)\n",
    "    ax2.set_ylabel('Tokens Used', color='gray')\n",
    "    \n",
    "    # Formatting\n",
    "    ax1.set_xlabel('Time (minutes)')\n",
    "    ax1.set_ylabel('Stats Value')\n",
    "    ax1.set_title(f'Gotchi Experiment: {model_name}')\n",
    "    ax1.set_ylim(-1, 6)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Action legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    action_elements = [Line2D([0], [0], marker=m, color='w', markerfacecolor=action_colors[a], \n",
    "                             markersize=10, label=f'{a}: {[\"Feed\", \"Play\", \"Sleep\", \"Quit\"][i]}')\n",
    "                      for i, (a, m) in enumerate(action_markers.items())]\n",
    "    ax1.legend(handles=action_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you've set your API key above!\n",
    "if not OPENAI_API_KEY and \"localhost\" not in str(client.base_url):\n",
    "    print(\"⚠️  Warning: No API key set! Please set OPENAI_API_KEY in the cell above.\")\n",
    "else:\n",
    "    # Create and run the experiment\n",
    "    print(f\"Starting Gotchi experiment with {MODEL}...\")\n",
    "    ag = AutoGotchi()\n",
    "    logs = ag.trial(duration_minutes=60)  # Run for 60 minutes of game time\n",
    "    print(f\"\\nExperiment complete! {len(logs)} actions taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "if 'logs' in locals() and logs:\n",
    "    plot_run(logs)\n",
    "else:\n",
    "    print(\"No data to plot. Make sure you've run the experiment above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Analyze the Results\n",
    "\n",
    "Look for patterns in your LLM's behavior:\n",
    "- Did it discover the hidden friendship stat?\n",
    "- How well did it balance the three visible stats?\n",
    "- Did it learn from its mistakes?\n",
    "\n",
    "Try different models and compare their performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View the LLM's reasoning for specific actions\n",
    "if 'logs' in locals() and logs:\n",
    "    print(\"First 3 reasoning steps:\")\n",
    "    for i in range(min(3, len(logs))):\n",
    "        print(f\"\\nStep {i+1} - Action: {logs[i]['action_selected']}\")\n",
    "        print(f\"Reasoning: {logs[i]['reasoning'][:200]}...\")\n",
    "else:\n",
    "    print(\"No logs available. Run the experiment first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
